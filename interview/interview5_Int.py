'''
简述 进程、线程、协程的区别 以及应用场景？
进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。
线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的)。
协程和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。
进程和其他两个的区别还是很明显的
协程和线程的区别是：协程避免了无意义的调度，由此可以提高性能，
但也因此，程序员必须自己承担调度的责任，同时，协程也失去了标准线程使用多CPU的能力。

堆与栈的区别很明显：
    1.栈内存存储的是局部变量而堆内存存储的是实体；
    2.栈内存的更新速度要快于堆内存，因为局部变量的生命周期很短；
    3.栈内存存放的变量生命周期一旦结束就会被释放，而堆内存存放的实体会被垃圾回收机制不定时的回收

进程：充分利用多CPU
线程：充分利用多核（达到真正的多任务并行）
协程：充分利用单核（充分挖掘不断提高性能的单核CPU的潜力。类比事件驱动和异步程序）。
      既可以利用异步优势，又可以避免反复系统调用，还有进程切换造成的开销。
协程存在的意义： 
  对于多线程应用，CPU通过切片的方式来切换线程间的执行，线程切换时需要耗时（保存状态，下次继续）。
  协程，则只使用一个线程，在一个线程中规定某个代码块执行顺序。
  协程能保留上一次调用时的状态，不需要像线程一样用回调函数，所以性能上会有提升。
缺点：本质是个单线程，不能利用到单个CPU的多个核。
线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作则是程序员。
切换开销（即调度和切换的时间）：进程 > 线程 > 协程

更加具象的解释：
  一开始大家想要同一时间执行那么三五个程序，大家能一块跑一跑。特别是UI什么的，别一上计算量比较大的玩意就跟死机一样。
于是就有了并发，从程序员的角度可以看成是多个独立的逻辑流。
内部可以是多cpu并行，也可以是单cpu时间分片，能快速的切换逻辑流，看起来像是大家一块跑的就行。

  但是一块跑就有问题了。我计算到一半，刚把多次方程解到最后一步，你突然插进来，我的中间状态咋办，我用来储存的内存被你覆盖了咋办？
所以跑在一个cpu里面的并发都需要处理上下文切换的问题。
进程就是这样抽象出来个一个概念，搭配虚拟内存、进程表之类的东西，用来管理独立的程序运行、切换。

  后来一电脑上有了好几个cpu，好咧，大家都别闲着，一人跑一进程。就是所谓的并行。

  因为程序的使用涉及大量的计算机资源配置，把这活随意的交给用户程序，非常容易让整个系统分分钟被搞跪，资源分配也很难做到相对的公平。
所以核心的操作需要陷入内核(kernel)，切换到操作系统，让老大帮你来做。

  因为程序的使用涉及大量的计算机资源配置，把这活随意的交给用户程序，非常容易让整个系统分分钟被搞跪，资源分配也很难做到相对的公平。
所以核心的操作需要陷入内核(kernel)，切换到操作系统，让老大帮你来做。

  如果连时钟阻塞、 线程切换这些功能我们都不需要了，自己在进程里面写一个逻辑流调度的东西。
那么我们即可以利用到并发优势，又可以避免反复系统调用，还有进程切换造成的开销，分分钟给你上几千个逻辑流不费力。
这就是用户态线程。

  从上面可以看到，实现一个用户态线程有两个必须要处理的问题：一是碰着阻塞式I\O会导致整个进程被挂起；
二是由于缺乏时钟阻塞，进程需要自己拥有调度线程的能力。如果一种实现使得每个线程需要自己通过调用某个方法，主动交出控制权。
那么我们就称这种用户态线程是协作式的，即是协程。

本质上协程就是用户空间下的线程。


GIL是什么？
由于物理上得限制，各CPU厂商在核心频率上的比赛已经被多核所取代。
为了更有效的利用多核处理器的性能，就出现了多线程的编程方式，而随之带来的就是线程间数据一致性和状态同步的困难。
即使在CPU内部的Cache也不例外，为了有效解决多份缓存之间的数据同步时各厂商花费了不少心思，也不可避免的带来了一定的性能损失。

    Python当然也逃不开，为了利用多核，Python开始支持多线程。而解决多线程之间数据完整性和状态同步的最简单方法自然就是加锁。
于是有了GIL这把超级大锁，而当越来越多的代码库开发者接受了这种设定后，
他们开始大量依赖这种特性（即默认python内部对象是thread-safe的，无需在实现时考虑额外的内存锁和同步操作）。

进程的状态：
    运行——>就绪：1，主要是进程占用CPU的时间过长，时间片用完；
                2，在采用抢先式优先级调度算法的系统中,当有更高优先级的进程要运行时，该进程就被迫让出CPU，该进程便由执行状态转变为就绪状态。
    就绪——>运行：运行的进程的时间片用完，调度就转到就绪队列中选择合适的进程分配CPU
    运行——>阻塞：正在执行的进程因发生某等待事件而无法执行，则进程由执行状态变为阻塞状态，如发生了I/O请求
    阻塞——>就绪: 进程所等待的事件已经发生，就进入就绪队列


Python中如何使用线程池和进程池？
concurent.future模块需要了解的
1.concurent.future模块是用来创建并行的任务，提供了更高级别的接口，为了异步执行调用
2.concurent.future这个模块用起来非常方便，它的接口也封装的非常简单
3.concurent.future模块既可以实现进程池，也可以实现线程池
4.模块导入进程池和线程池
from  concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
还可以导入一个Executor,但是你别这样导，这个类是一个抽象类
抽象类的目的是规范他的子类必须有某种方法（并且抽象类的方法必须实现），但是抽象类不能被实例化
5.
  p = ProcessPoolExecutor(max_works)对于进程池如果不写max_works：默认的是cpu的数目,默认是4个
  p = ThreadPoolExecutor(max_works)对于线程池如果不写max_works：默认的是cpu的数目*5
6.如果是进程池，得到的结果如果是一个对象。我们得用一个.get()方法得到结果
  但是现在用了concurent.future模块，我们可以用obj.result方法
  p.submit(task,i)  #相当于apply_async异步方法
  p.shutdown()  #默认有个参数wite=True (相当于close和join)
我们知道多进程可以利用多核CPU进行并行计算，但多进程的资源开销很大，因此不可能无限开。
举个栗子，假设有100块砖，是请5个人去搬，还是请100个人去搬？
答案是5个人，100个人工钱太贵了。。。这就和开多进程是一样的道理。
那如何限制进程数量呢？通过进程池。
Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；
但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，就重用进程池中的进程。
from multiprocessing import Pool
import os, time

def foo(i):
    print('进程【%s】,id-->%s'%(i,os.getpid()))
    time.sleep(2)

if __name__ == '__main__':
    pool = Pool(4)  # 指定进程池大小，默认为CPU核心数
    for i in range(1,10):
        pool.apply_async(func=foo, args=(i,)) # 如果没有返回计算结果就不用赋值

    pool.close()    # 关闭进程池
    pool.join() # 等待进程池内任务处理完,否则主进程走完就结束了

进程池的方法： 
pool = Pool() 创建进程池 
pool.apply(func=func_name, args=()) 从进程池里取一个进程并执行 
pool.apply_async(func=func_name, args=()) apply的异步版本 
apply() 和apply_async的返回值是结果对象obj, 通过obj.get()可以收集结果。 
pool.close() 关闭进程池 
pool.terminate() 终止所有工作进程 
pool.join() 主进程等待所有子进程执行完毕，必须在close或terminate之后

python3中针对进程池和线程池提供了更易用的接口。
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor # 导入
import random, time

def get_html(url):
    print('请求%s' % url)
    time.sleep(random.randint(1, 3))
    return '%s html......' % url

def parse_html(res):
    html = res.result()  # 任务完成返回的是结果对象，回调函数接收该对象通过.result()获取真实结果
    print(html)

if __name__ == "__main__":
    start = time.time()

    pool = ProcessPoolExecutor(4)  # 默认进程池大小等于CPU核心数
    # pool = ThreadPoolExecutor(6)  # 线程池一般20-30左右足够，可以根据任务适当调整

    urls = ['url_1', 'url_2', 'url_3', 'url_4', 'url_5', 'url_6']
    for url in urls:
        pool.submit(get_html, url).add_done_callback(parse_html)  # 提交任务到进程池并指定回调函数，
                                                                    即子进程执行结果出来后，通知主进程处理结果（执行回调函数）
    pool.shutdown(wait=True)  # 关闭进程等待任务结束；这里主要是为了阻塞主进程，统计时间用

    end = time.time()
    print('一共耗时：', end - start)

    """
    请求url_1
    请求url_2
    请求url_3
    请求url_4
    请求url_5
    url_1 html......
    请求url_6
    url_4 html......
    url_6 html......
    url_5 html......
    url_2 html......
    url_3 html......
    一共耗时： 3.2524783611297607
    """
    可以看到，如果要用线程，只需要实例化线程池对象即可。使用非常方便


threading.local的作用？

#coding=utf-8
import threading
# 创建全局ThreadLocal对象:
localVal = threading.local()
localVal.val = "Main-Thread"
def process_student():
    print '%s (in %s)' % (localVal.val, threading.current_thread().name)
def process_thread(name):
    #赋值
    localVal.val = name
    process_student()
t1 = threading.Thread(target= process_thread, args=('One',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Two',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
print localVal.val

打印结果：
One (in Thread-A)
Two (in Thread-B)
Main-Thread

threading.local()这个方法的特点用来保存一个全局变量，但是这个全局变量只有在当前线程才能访问，
localVal.val = name这条语句可以储存一个变量到当前线程，如果在另外一个线程里面再次对localVal.val进行赋值，
那么会在另外一个线程单独创建内存空间来存储，也就是说在不同的线程里面赋值 不会覆盖之前的值，因为每个
线程里面都有一个单独的空间来保存这个数据,而且这个数据是隔离的，其他线程无法访问
这个东西可以用在那些地方呢，比如下载，现在都是多线程下载了，就像酷狗那样，可以同时下载很多首歌曲，那么
就可以利用这个方法来保存每个下载线程的数据，比如下载进度，下载速度之类的
所以,如果你在开发多线程应用的时候  需要每个线程保存一个单独的数据供当前线程操作，可以考虑使用这个方法，简单有效
其实这样的功能还有很多种方法可以实现，比如我们在主线程实例化一个dict对象，然后用线程的名字作为key，因为线程之间可以共享数据，
所以也可以实现相同功能，并且灵活性更多，不过代码就没那么优雅简洁了
